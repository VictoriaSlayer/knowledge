https://habr.com/ru/companies/gaz-is/articles/884410/
**LLM** (Large Language Model, большая языковая модель) — это тип модели машинного обучения (или тип модели глубокого обучения), основанный на больших объёмах данных, который может выполнять любые задачи по обработке естественного языка (Natural Language Processing, NLP).

Более простыми словами это нейронная сеть с крайне большим количеством изменяемых параметров, которая позволяет решать задачи по обработке и генерации текста. Чаще всего реализована в виде диалогового агента, с которым можно общаться в разговорной форме.

**NLP** — область информатики, который включает в себя предоставление компьютерам возможности интерпретировать и воспроизводить язык. В то же время NLP охватывает несколько областей обучения, включая лингвистику, информатику, статистику и искусственный интеллект. Основные сложности NLP заключаются в генерации и интерпретации человеческой речи, так как она достаточно двусмысленна и иногда даже люди, зная контекст, с трудом понимают, что имеется в виду. Например:
![[Pasted image 20251015104816.png]]
### 1.1 Основные понятия и параметры LLM

**Промпт** (запрос) — это вводимые пользователем данные, которые модель использует для ответа.

**Токены** — дискретные символы, слова и другие фрагменты текста, которые используются для представления данных в структурированном формате.

**Токенизация** — это процесс преобразования данных в отдельные единицы, называемые токенами (рис. 4, рис. 5).

Как показано на рисунках 4 и 5, токеном может быть как одно слово, так и один символ, а количество токенов в фразе на русском языке используется гораздо большее, чем на английском.

![Рис. 4 Токенизация на английском](https://habrastorage.org/r/w1560/getpro/habr/upload_files/f46/87a/579/f4687a57979d18c1e22afee7d7e98c5d.jpg "Рис. 4 Токенизация на английском")

Рис. 4 Токенизация на английском

![Рис. 5 Токенизация на русском](https://habrastorage.org/r/w1560/getpro/habr/upload_files/4d4/417/686/4d4417686c673704693832179c222130.jpg "Рис. 5 Токенизация на русском")

Рис. 5 Токенизация на русском

**Контекстное окно** — количество токенов, которые можно передать модели за раз (эквивалентно RAM в памяти компьютера).

**Эмбеддинги** — кодирование текста в смысл в виде вектора длины для базовой модели (сам вектор представляет собой смысл); или иначе — векторное представление слов в виде набора токенов.

**Embedding model** (модель встраивания) — тип LLM, который преобразует данные в векторы (массивы или группы чисел).

**Базовая модель** — это нейросеть, обученная на большом объёме данных, которую можно настроить для решения каких-либо задач.

**Температура** — параметр от 0 до 1, который влияет на креативность модели. При температуре близкой к 0 модель стремится дать более точный результат, при близкой к 1 выводит слова, которые менее часто встречались в обучающей выборке.

**Top-p** — выбор токенов из вариантов с наибольшей вероятностью; сумма их вероятностей определяет выбор модели.

Например, если p установлено равным 0,15, модель выберет такие токены, как «Юнайтед» и «Нидерланды», поскольку их вероятности составляют в сумме 14,7%, меньше 0,15, а «Чехию» уже проигнорирует (рис. 6). Чем ниже значение p, тем более стандартными являются ответы, генерируемые моделью. Общая рекомендация — изменить либо температуру, либо top-p, но не то и другое одновременно.

![Рис. 6 Top-p (пример)](https://habrastorage.org/r/w1560/getpro/habr/upload_files/635/9c2/5c7/6359c25c7f142d85c36d18f8b5e73c21.jpg "Рис. 6 Top-p (пример)")

Рис. 6 Top-p (пример)

**Top-k** — выбор следующего токена из списка токенов с наибольшим k, которые отсортированы по их вероятности (рис. 7). Например, если для k установлено значение 3, модель в соответствии с температурой выберет один из трёх лучших вариантов.

![Рис. 7 Top-k (пример)](https://habrastorage.org/r/w1560/getpro/habr/upload_files/1b0/79c/5e9/1b079c5e96dea2f7a4ad3049d09951fe.jpg "Рис. 7 Top-k (пример)")

Рис. 7 Top-k (пример)